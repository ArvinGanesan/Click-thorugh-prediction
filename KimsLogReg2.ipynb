{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W261 Final Project\n",
    "\n",
    "#### *Anusha Munjuluri, Arvindh Ganesan, Kim Vignola, Christina Papadimitriou*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"final_project\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Question Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Background \n",
    "\n",
    "The following analysis is based on a Kaggle dataset from Criteo, an internet advertising company focused on retargeting. Criteo's goal is to increase online clickthrough rates among consumers who have previously visited an advertiser's website. This information will be used by Criteo to more efficiently provide the right ads to the right people. Optimizing the retargeting process not only helps advertisers become more efficient in terms of how they spend their dollars, but also it reduces clutter for consumers who do not want to be \"followed\" by ads for irrelevant products (or ones they may have already purchased!).\n",
    "\n",
    "There are 13 numerical features and 26 categorical features in this dataset. Our goal is to create a model that will most accurately predict clickthroughs (label = 1). It is likely that these features represent characterstics about consumer behavior (history of clickthroughs, site visitiation, etc.), the ads themselves (product, creative approach, placement, etc.) and general metrics such as the date the ad was published. Since there is no visibility into what each feature represents, however, our challenge is to make our predictions based on the data alone. With over 6 million records, this will require a scalable approach.\n",
    "\n",
    "### Key Questions\n",
    "\n",
    "* Which machine learning approach not only provides the highest accuracy in predicting clickthroughs, but is also scalable enough to be useful in a production environment? As internet patterns and product choices change rapidly, the ideal model should be updated daily to update the following day's retargeting model.  - Note: not sure I fully answered this piece:  *** Preview what level of performanceyour model would need to achieve to be practically useful ***\n",
    "\n",
    "* Which features are most important in predicting clickthroughs? Having this information can help Criteo focus on the metrics that are most critical to their product.\n",
    "\n",
    "* With 39 features, there is a high risk of overfitting. We should identify a model that provides an optimal tradeoff between bias and variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithm Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the data\n",
    "!head -n 1 data/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "fullTrainRDD = sc.textFile('data/train.txt')\n",
    "testRDD = sc.textFile('data/test.txt')\n",
    "\n",
    "FIELDS = ['I1','I2','I3','I4','I5','I6','I7','I8','I9','I10','I11','I12','I13',\n",
    "          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n",
    "          'C15','C16','C17','C18','C19','C20','C21','C22','C23','C24','C25','C26','Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in train data: 45840617 ...\n",
      "Number of records in test data: 6042135 ...\n"
     ]
    }
   ],
   "source": [
    "# number of rows in train/test data\n",
    "print(f\"Number of records in train data: {fullTrainRDD.count()} ...\")\n",
    "print(f\"Number of records in test data: {testRDD.count()} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... held out 9167871 records for evaluation and assigned 36672746 for training.\n"
     ]
    }
   ],
   "source": [
    "# Generate 80/20 (pseudo)random train/test split \n",
    "trainRDD, heldOutRDD = fullTrainRDD.randomSplit([0.8,0.2], seed = 1)\n",
    "print(f\"... held out {heldOutRDD.count()} records for evaluation and assigned {trainRDD.count()} for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "toyRDD_train, trainRDD2 = trainRDD.randomSplit([0.001,0.999], seed = 2)\n",
    "toyRDD_test, mainRDD_test = heldOutRDD.randomSplit([0.001,0.999], seed = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\t5\\t2\\t\\t\\t1382\\t17\\t78\\t25\\t76\\t0\\t9\\t\\t\\t05db9164\\t942f9a8d\\t56472604\\t53a5d493\\t25c83c98\\t\\t49b74ebc\\t6c41e35e\\ta73ee510\\te113fc4b\\tc4adf918\\t08531bcb\\t85dbe138\\t1adce6ef\\tae97ecc3\\t76b06ec3\\te5ba7672\\t1f868fdd\\t9437f62f\\ta458ea53\\tff4c70b8\\t\\t32c7478e\\tda89b7d5\\t7a402766\\tc7beb94e']"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#toyRDD_train.take(1)\n",
    "toyRDD_train.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def parse(line):\n",
    "    \"\"\"\n",
    "    Map line --> tuple of (features, label)\n",
    "    \"\"\"\n",
    "    fields = np.array(line.split('\\t'))\n",
    "    features,label = fields[1:14], fields[0]\n",
    "    return(features, label)\n",
    "\n",
    "def edit_data_types(line):\n",
    "    \"\"\"\n",
    "    Map tuple of (features, label) --> tuple of (formated features, label)\n",
    "    \n",
    "    * '' is replaced with 'null'\n",
    "    * numerical fields are converted to integers\n",
    "    \"\"\"\n",
    "    features, label = line[0], line[1]\n",
    "    formated_features = []\n",
    "    for i, value in enumerate(features):\n",
    "        if value == '':\n",
    "            formated_features.append(np.nan)\n",
    "        else:\n",
    "            if i < 13:\n",
    "                formated_features.append(float(value)) \n",
    "            else:\n",
    "                formated_features.append(float(value))\n",
    "    return (formated_features, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainRDDCached = trainRDD.map(parse).map(edit_data_types).cache()\n",
    "toyRDD_train1 = toyRDD_train.map(parse).map(edit_data_types)\n",
    "toyRDD_test2 = toyRDD_test.map(parse).map(edit_data_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36451\n"
     ]
    }
   ],
   "source": [
    "print(toyRDD_train1.count())\n",
    "#print(toyRDD_test1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.array(toyRDD_train1.map(lambda x: np.append(x[0], [x[1]])).takeSample(False, 1000))\n",
    "sample_df = pd.DataFrame(np.array(sample), columns = ['I1','I2','I3','I4','I5','I6','I7','I8','I9','I10','I11','I12','I13', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = (['I1','I2','I3','I4','I5','I6','I7','I8','I9','I10','I11','I12','I13', 'Label'])\n",
    "#columns = ['I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'I10', 'I11', 'I12', 'I13']\n",
    "sample_numeric = sample_df.reindex(columns=columns)\n",
    "sample_numeric[columns] = sample_numeric[columns].astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.4376199616122842, 90.14, 19.89001264222503, 6.916349809885932, 17246.35677352637, 124.63518758085381, 14.970772442588727, 12.226226226226226, 106.30480167014613, 0.6084452975047985, 2.5260960334029225, 0.6375, 8.043092522179975]\n",
      "[10.984391530315705, 323.5902044252885, 79.83880276011938, 8.267584408314738, 55493.864000837486, 288.36392470232033, 64.45374433092296, 12.987052645203304, 205.91002503807812, 0.6956443052357069, 5.146988458022784, 2.40265694943466, 10.356297402365538]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Get means and standard deviations. Ideally we should do this in the RDD vs. pandas\"\"\"\n",
    "\n",
    "means = []\n",
    "stdevs = []\n",
    "\n",
    "for i in sample_numeric.columns[0:13]:\n",
    "    mean = np.nanmean(sample_numeric[i])\n",
    "    means.append(mean)\n",
    "    std = np.nanstd(sample_numeric[i])\n",
    "    stdevs.append(std)\n",
    "        \n",
    "print(means)\n",
    "print(stdevs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.000e+00 2.000e+00 5.000e+00 4.000e+00 1.516e+03 1.900e+01 5.000e+00\n",
      " 6.000e+00 4.000e+01 1.000e+00 2.000e+00 0.000e+00 3.000e+00 1.000e+00]\n",
      "[1.000e+00 3.000e+00 6.000e+00 4.000e+00 3.506e+03 4.300e+01 3.000e+00\n",
      " 7.000e+00 3.900e+01 0.000e+00 1.000e+00 0.000e+00 4.000e+00 0.000e+00]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Get medians for each class. Ideally we should do this in the RDD vs. pandas\"\"\"\n",
    "\n",
    "median1 = np.array(sample_numeric[sample_numeric['Label'] == 1.0].median().tolist())\n",
    "print(median1)\n",
    "\n",
    "median0 = np.array(sample_numeric[sample_numeric['Label'] == 0.0].median().tolist())\n",
    "print(median0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def parse(line):\n",
    "    \"\"\"\n",
    "    Map line --> tuple of (features, label)\n",
    "    \"\"\"\n",
    "    fields = np.array(line.split('\\t'))\n",
    "    features,label = fields[1:14], fields[0]\n",
    "    return(features, label)\n",
    "\n",
    "def update_nans(line):\n",
    "    \"\"\"\n",
    "    Replace missing values with meidans for each label    \n",
    "    \"\"\"\n",
    "    \n",
    "    #median1 = np.array([2.0, 3.5, 4.0, 4.0, 1362.0, 13.5, 8.0, 7.0, 42.5, 1.0, 2.0, 0.0, 3.0, 1.0])\n",
    "    #median0 = np.array([0.0, 2.0, 7.0, 5.0, 3539.0, 46.5, 2.0, 8.0, 38.5, 0.0, 1.0, 0.0, 5.0, 0.0])\n",
    "        \n",
    "    features, label = line[0], float(line[1])\n",
    "    formated_features = []\n",
    "    for i, value in enumerate(features):\n",
    "        if value == '' and label == 1.0:\n",
    "            formated_features.append(float(median1[i]))\n",
    "        elif value == '' and label == 0.0:\n",
    "            formated_features.append(float(median0[i]))\n",
    "        else:\n",
    "            if i < 13:\n",
    "                formated_features.append(float(value)) \n",
    "            else:\n",
    "                formated_features.append(value)\n",
    "    return (formated_features, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "toyRDDCached_train = toyRDD_train.map(parse).map(update_nans)\n",
    "toyRDDCached_test = toyRDD_test.map(parse).map(update_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanClickthroughs = toyRDDCached_train.values().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25475295602315423"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanClickthroughs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([1.0, 3.0, 5.0, 2.0, 2.0, 0.0, 55.0, 15.0, 23.0, 1.0, 11.0, 0.0, 0.0], 1.0)]"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toyRDDCached_test.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is what I was working on to flatten outliers..\n",
    "\n",
    "\n",
    "featureMeans = np.array(means)\n",
    "featureStdev = np.array(stdevs)\n",
    "upper_bounds = np.array(featureMeans + (featureStdev * 3))\n",
    "lower_bounds = np.array(featureMeans - (featureStdev * 3))\n",
    "# print(\"feature means\", featureMeans)\n",
    "# print(\"feature std devs\", featureStdev)\n",
    "# print(\"upper_bounds\", upper_bounds)\n",
    "# print(\"lower_bounds\", lower_bounds)\n",
    "\n",
    "def flatten_outliers(line):\n",
    "    features = line[0]\n",
    "    labels = line[1]\n",
    "    for i in range(features):\n",
    "        for j in range(upper_bounds):\n",
    "            if i > j:\n",
    "                features[i] = j\n",
    "    for i in range(features):\n",
    "        for k in range(lower_bounds):\n",
    "            if i < k:\n",
    "                features[i] = k\n",
    "    return features, labels\n",
    "            \n",
    "# testRDD = toyRDDCached_train.mapValues(lambda x: x == ).take(1)\n",
    "#testRDD.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(dataRDD):\n",
    "    \"\"\"Standardize the data\"\"\"\n",
    "\n",
    "    sc.broadcast(means)\n",
    "    sc.broadcast(stdevs)\n",
    "    \n",
    "    featureMeans = np.array(means)\n",
    "    featureStdev = np.array(stdevs)\n",
    "       \n",
    "    normedRDD = dataRDD.map(lambda x: ((x[0]-featureMeans)/featureStdev, x[1]))\n",
    "    \n",
    "    return normedRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize both the test and training sets\n",
    "normedRDD_train = standardize(toyRDDCached_train).cache()\n",
    "normedRDD_test = standardize(toyRDDCached_test).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 0.14223638, -0.27238155, -0.18650095, -0.35274509, -0.28587587,\n",
       "         -0.37326163,  0.97789862,  0.98357758, -0.14717497, -0.87465001,\n",
       "          1.25780425, -0.26533126, -0.48695903]), 1.0)]"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normedRDD_train.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EDA & Discussion of Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statitics\n",
    "sample_df.iloc[:,0:21].describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics\n",
    "sample_df.iloc[:,21:39].describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at histograms for each feature \n",
    "sample_numeric.hist(figsize=(23,15), bins=15)\n",
    "#sample_numeric[FIELDS[:-1]].hist(figsize=(15,15), bins=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part b -  plot boxplots of each feature vs. the outcome\n",
    "\n",
    "fig, ax_grid = plt.subplots(5, 3, figsize=(23,15))\n",
    "y = sample_df['Label']\n",
    "for idx, feature in enumerate(FIELDS[0:13]):\n",
    "    x = sample_numeric[feature]\n",
    "    sns.boxplot(x, y, ax=ax_grid[idx//3][idx%3], orient='h', linewidth=.5)\n",
    "    ax_grid[idx//3][idx%3].invert_yaxis()\n",
    "fig.suptitle(\"BoxPlots by Label\", fontsize=15, y=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at correrlations across features\n",
    "\n",
    "corr = sample_numeric[FIELDS[:13]].corr()\n",
    "fig, ax = plt.subplots(figsize=(15, 13))\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "cmap = sns.diverging_palette(10, 240, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, center=0, linewidths=.5)\n",
    "plt.title(\"Correlations between features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following is a homegrown implementation of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the baseline model\n",
    "BASELINE_1 = np.array([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "BASELINE_2 = np.array([meanClickthroughs, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log loss for Logisitc Regression\n",
    "def LRLoss(cachedRDD, W):\n",
    "    \n",
    "    augmentedData = cachedRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "\n",
    "    loss = augmentedData.map(lambda x: -x[1] * np.log(1/ (1 + np.exp(-(np.dot(x[0],W))))) - (1-x[1]) * np.log(1-(1/ (1 + np.exp(-(np.dot(x[0],W))))))).mean()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss using 0 for baseline 1.0585087314950672\n",
      "Train Loss using meanClickthroughs as baseline 0.7637151310692348\n"
     ]
    }
   ],
   "source": [
    "#Using meanClickthroughs as our bias term improves loss significantly (it will be used in our baseline)\n",
    "\n",
    "train_loss_baseline1 = LRLoss(normedRDD_train, BASELINE_1)\n",
    "print(\"Train Loss using 0 for baseline\", train_loss_baseline1)\n",
    "\n",
    "train_loss_baseline2 = LRLoss(normedRDD_train, BASELINE_2)\n",
    "print(\"Train Loss using meanClickthroughs as baseline\", train_loss_baseline2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE = np.array([meanClickthroughs, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the gradient for logistic regression (one step)\n",
    "\n",
    "def GDUpdate(dataRDD, W, learningRate):\n",
    "    \n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1])).cache()\n",
    "        \n",
    "    grad = augmentedData.map(lambda x: np.dot(x[0], ((1/ (1 + np.exp(-(np.dot(x[0],W))))) - x[1]))).mean()\n",
    "\n",
    "    new_model = W - (learningRate * grad)\n",
    "\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE:  Loss = 0.7637151310692348\n",
      "----------\n",
      "STEP: 1\n",
      "Loss: 0.746340881015326\n",
      "Model: [0.224, 0.006, 0.001, 0.001, -0.001, -0.005, -0.001, 0.003, -0.003, 0.001, 0.025, 0.006, 0.008, -0.004]\n",
      "----------\n",
      "STEP: 2\n",
      "Loss: 0.7302502404560913\n",
      "Model: [0.194, 0.012, 0.003, 0.002, -0.001, -0.01, -0.001, 0.007, -0.006, 0.002, 0.049, 0.012, 0.015, -0.006]\n",
      "----------\n",
      "STEP: 3\n",
      "Loss: 0.7152903868179628\n",
      "Model: [0.165, 0.018, 0.004, 0.003, -0.002, -0.015, -0.002, 0.009, -0.008, 0.003, 0.072, 0.017, 0.022, -0.009]\n"
     ]
    }
   ],
   "source": [
    "# Review Gradient Descent steps\n",
    "nSteps = 3\n",
    "model = BASELINE\n",
    "#learningRate = .1\n",
    "print(f\"BASELINE:  Loss = {LRLoss(normedRDD_train,model)}\")\n",
    "for idx in range(nSteps):\n",
    "    print(\"----------\")\n",
    "    print(f\"STEP: {idx+1}\")\n",
    "    model = GDUpdate(normedRDD_train, model, .1)\n",
    "    loss = LRLoss(normedRDD_train, model)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Model: {[round(w,3) for w in model]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one Gradient Descent update with lasso regularization\n",
    "\n",
    "def GDUpdate_Lasso(dataRDD, W, learningRate, regParam):\n",
    "\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1])).cache()\n",
    "    #new_model = None    \n",
    "    wReg = np.sign(W)\n",
    "    wReg[0] = 0  #set bias to zero\n",
    "       \n",
    "    grad = augmentedData.map(lambda x: np.dot(x[0], ((1/ (1 + np.exp(-(np.dot(x[0],W))))) - x[1]))).mean() + (wReg * regParam)\n",
    "    new_model = W - (learningRate * grad)\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE:  Loss = 0.7637151310692348\n",
      "----------\n",
      "STEP: 1\n",
      "Loss: 0.746340881015326\n",
      "Model: [0.224, 0.006, 0.001, 0.001, -0.001, -0.005, -0.001, 0.003, -0.003, 0.001, 0.025, 0.006, 0.008, -0.004]\n",
      "----------\n",
      "STEP: 2\n",
      "Loss: 0.7365672898524418\n",
      "Model: [0.194, 0.002, -0.007, -0.008, 0.009, -0.0, 0.009, -0.003, 0.004, -0.008, 0.039, 0.002, 0.005, 0.004]\n",
      "----------\n",
      "STEP: 3\n",
      "Loss: 0.7239573716740647\n",
      "Model: [0.165, -0.002, 0.004, 0.007, -0.003, 0.005, -0.002, 0.01, -0.01, 0.004, 0.052, -0.003, 0.002, -0.011]\n",
      "----------\n",
      "STEP: 4\n",
      "Loss: 0.7119064419227186\n",
      "Model: [0.137, 0.014, -0.004, -0.004, 0.007, -0.01, 0.007, 0.003, -0.002, -0.005, 0.066, 0.013, -0.001, -0.003]\n",
      "----------\n",
      "STEP: 5\n",
      "Loss: 0.7015280179148989\n",
      "Model: [0.109, 0.009, 0.007, 0.009, -0.004, -0.005, -0.004, -0.004, 0.005, 0.006, 0.078, 0.008, 0.016, 0.004]\n"
     ]
    }
   ],
   "source": [
    "# Take a look at a few Gradient Descent steps\n",
    "nSteps = 5\n",
    "model = BASELINE\n",
    "#learningRate = .1\n",
    "#regParam = .1\n",
    "print(f\"BASELINE:  Loss = {LRLoss(normedRDD_train,model)}\")\n",
    "for idx in range(nSteps):\n",
    "    print(\"----------\")\n",
    "    print(f\"STEP: {idx+1}\")\n",
    "    model = GDUpdate_Lasso(normedRDD_train, model, .1, .1)\n",
    "    loss = LRLoss(normedRDD_train, model)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Model: {[round(w,3) for w in model]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(dataRDD, W, threshold):\n",
    "    \"\"\"Assess peformance of current models\"\"\"\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "        \n",
    "    sc.broadcast(W)\n",
    "    sc.broadcast(threshold)\n",
    "\n",
    "    def predict_label(line):\n",
    "        features = line[0]\n",
    "        label = line[1]\n",
    "        pred = float(0.0)\n",
    "        z = np.dot(features,W)\n",
    "        prob = float(1 / (1 + np.exp(-z)))\n",
    "        if prob > threshold:\n",
    "            pred = float(1.0)\n",
    "        else:\n",
    "            pred = float(0.0)\n",
    "        return label, pred\n",
    "        \n",
    "    def map_accuracy(line):\n",
    "        label_actual = line[0]        \n",
    "        label_pred = line[1]\n",
    "        \n",
    "        if label_actual == 1.0 and label_pred == 1.0:\n",
    "            return \"TP\", 1.0\n",
    "        elif label_actual == 1.0 and label_pred == 0.0:\n",
    "            return \"FN\", 1.0\n",
    "        elif label_actual == 0.0 and label_pred == 0.0:\n",
    "            return \"TN\", 1.0\n",
    "        else:\n",
    "            if label_actual == 0.0 and label_pred == 1.0:\n",
    "                return \"FP\", 1.0\n",
    "    \n",
    "    scores = augmentedData.map(predict_label).map(map_accuracy).reduceByKey(lambda x, y: x + y)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_model = [0.773, 0.006, 0.008, 0.008, 0.005, -0.009, 0.006, 0.005, -0.007, -0.007, 0.105, -0.001, 0.011, -0.011]\n",
    "# train_scores_test = get_scores(normedRDD_train, new_model, .5).collect()\n",
    "# print(train_scores)\n",
    "test_model = GDUpdate_Lasso(normedRDD_train, BASELINE, .1, .1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('FP', 27165.0), ('TP', 9286.0)]"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Get_Scores = get_scores(normedRDD_train, BASELINE, .5).collect()\n",
    "Get_Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7666181997750404"
      ]
     },
     "execution_count": 708,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy =  assess_performance(Get_Scores)\n",
    "Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess performance of current models. Takes \"train_scores\" as input.\n",
    "\n",
    "def assess_performance(scores):\n",
    "\n",
    "    TP, FN, TN, FP = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    for i in train_scores:\n",
    "        if i[0] == 'TP':\n",
    "            TP = i[1]\n",
    "        elif i[0] == 'FN':\n",
    "            FN = i[1]\n",
    "        elif i[0] == 'TN':\n",
    "            TN = i[1]\n",
    "        else:\n",
    "            if i[0] == 'FP':\n",
    "                FP = i[1]\n",
    "\n",
    "    if TP != 0.0:\n",
    "        precision = TP/(TP + FP)\n",
    "        recall = TP/(TP + FN)\n",
    "        f1_score = 2*((precision*recall)/(precision+recall))\n",
    "        accuracy = (TP+TN)/(TP+TN+FP+FN)   \n",
    "    else:\n",
    "        precision = 0.0\n",
    "        recall = 0.0\n",
    "        f1_score = 0.0\n",
    "        accuracy = 0.0\n",
    "\n",
    "#     print(\"True Positives=\", TP)\n",
    "#     print(\"False Negatives=\", FN)\n",
    "#     print(\"True Negatives=\", TN)\n",
    "#     print(\"False Positives=\", FP)\n",
    "#     print(\"Precision=\", precision)\n",
    "#     print(\"Recall=\", recall)\n",
    "#     print(\"F1_score=\", f1_score)\n",
    "#     print('Accuracy=', accuracy)\n",
    "    \n",
    "    return accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Logistic Regression gradient descent function and capture accuracy for each model\n",
    "def GradientDescent(trainRDD, testRDD, wInit, nSteps = 10, \n",
    "                    learningRate = 0.1, verbose = True):\n",
    "    \"\"\"\n",
    "    Perform nSteps iterations of Logistic Regression gradient descent and \n",
    "    track loss on a test and train set. Return lists of\n",
    "    test/train loss and the models themselves.\n",
    "    \"\"\"\n",
    "    # initialize lists to track model performance\n",
    "    train_history, test_history, model_history = [], [], []\n",
    "    \n",
    "    # perform n updates & compute test and train loss after each\n",
    "    model = wInit\n",
    "    for idx in range(nSteps): \n",
    "        \n",
    "        model = GDUpdate_Lasso(trainRDD, model, .1, .1)\n",
    "        training_loss = LRLoss(trainRDD, model)\n",
    "        test_loss = LRLoss(testRDD, model)\n",
    "        \n",
    "        #training_scores = get_scores(trainRDD, model, .5)\n",
    "        #testing_scores = get_scores(testRDD, model, .5)\n",
    "        \n",
    "        #train_accuracy = assess_performance(training_scores)\n",
    "        #test_accuracy = assess_performance(testing_scores)\n",
    "        \n",
    "        # keep track of accuracy for plotting\n",
    "        train_history.append(training_loss)\n",
    "        test_history.append(test_loss)\n",
    "        model_history.append(model)\n",
    "        \n",
    "        # console output \n",
    "        if verbose:\n",
    "            print(\"----------\")\n",
    "            print(f\"STEP: {idx+1}\")\n",
    "            print(f\"training accuracy: {training_loss}\")\n",
    "            print(f\"test accuracy: {test_loss}\")\n",
    "            print(f\"Model: {[round(w,3) for w in model]}\")\n",
    "    return train_history, test_history, model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "STEP: 1\n",
      "training accuracy: 0.746340881015326\n",
      "test accuracy: 0.7438235953159592\n",
      "Model: [0.224, 0.006, 0.001, 0.001, -0.001, -0.005, -0.001, 0.003, -0.003, 0.001, 0.025, 0.006, 0.008, -0.004]\n",
      "----------\n",
      "STEP: 2\n",
      "training accuracy: 0.7365672898524418\n",
      "test accuracy: 0.7345152531544128\n",
      "Model: [0.194, 0.002, -0.007, -0.008, 0.009, -0.0, 0.009, -0.003, 0.004, -0.008, 0.039, 0.002, 0.005, 0.004]\n",
      "----------\n",
      "STEP: 3\n",
      "training accuracy: 0.7239573716740647\n",
      "test accuracy: 0.7221483167249952\n",
      "Model: [0.165, -0.002, 0.004, 0.007, -0.003, 0.005, -0.002, 0.01, -0.01, 0.004, 0.052, -0.003, 0.002, -0.011]\n",
      "\n",
      "... trained 3 iterations in 58.11057186126709 seconds\n"
     ]
    }
   ],
   "source": [
    "# run 50 iterations (RUN THIS CELL AS IS)\n",
    "wInit = BASELINE\n",
    "#trainRDD, testRDD = normedRDD.randomSplit([0.8,0.2], seed = 2018)\n",
    "start = time.time()\n",
    "Train_Loss, Test_Loss, models = GradientDescent(normedRDD_train, normedRDD_test, wInit, nSteps = 3)\n",
    "print(f\"\\n... trained {len(models)} iterations in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot error curves - RUN THIS CELL AS IS\n",
    "def plotErrorCurves(trainLoss, testLoss, title = None):\n",
    "    \"\"\"\n",
    "    Helper function for plotting.\n",
    "    Args: trainLoss (list of MSE) , testLoss (list of MSE)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,1,figsize = (16,8))\n",
    "    x = list(range(len(train_Accuracy)))[1:]\n",
    "    ax.plot(x, train_Accuracy[1:], 'k--', label='Training Accuracy')\n",
    "    ax.plot(x, test_Accuracy[1:], 'r--', label='Test Accuracy')\n",
    "    ax.legend(loc='upper right', fontsize='x-large')\n",
    "    plt.xlabel('Number of Iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Application of Course Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
